apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-chatbot-backend
  namespace: default
  labels:
    app: llm-chatbot
    component: backend
    version: v1
    environment: cloud
spec:
  replicas: 1  # Reduced to 1 for Ollama resource requirements
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: llm-chatbot
      component: backend
  template:
    metadata:
      labels:
        app: llm-chatbot
        component: backend
        version: v1
        environment: cloud
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: llm-chatbot-service-account
      securityContext:
        runAsNonRoot: false  # Ollama needs root access
        fsGroup: 1000
      initContainers:
      # Download models before starting main containers
      - name: model-downloader
        image: ollama/ollama:latest
        command: 
        - /bin/sh
        - -c
        - |
          ollama serve &
          sleep 10
          echo "Downloading phi model..."
          ollama pull phi
          echo "Downloading llama2:7b model..."
          ollama pull llama2:7b
          echo "Model download complete"
          pkill ollama
        volumeMounts:
        - name: ollama-data
          mountPath: /root/.ollama
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
      containers:
      # Ollama sidecar container
      - name: ollama
        image: ollama/ollama:latest
        ports:
        - containerPort: 11434
          name: ollama-http
        env:
        - name: OLLAMA_HOST
          value: "0.0.0.0"
        volumeMounts:
        - name: ollama-data
          mountPath: /root/.ollama
        resources:
          requests:
            memory: "2Gi"     # Increased for model storage
            cpu: "1000m"      # Increased for model inference
          limits:
            memory: "4Gi"     # Increased for model storage
            cpu: "2000m"      # Increased for model inference
        livenessProbe:
          httpGet:
            path: /api/version
            port: ollama-http
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /api/version
            port: ollama-http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
      # Main application container
      - name: backend
        image: gcr.io/scalable-llm-chatbot/llm-chatbot-backend:latest
        imagePullPolicy: Always  # Always pull latest for cloud
        ports:
        - containerPort: 8000
          name: http
          protocol: TCP
        env:
        - name: ENVIRONMENT
          value: "cloud"
        - name: PORT
          value: "8000"
        - name: LLM_MODEL_PROVIDER
          value: "ollama"  # Use Ollama in cloud
        - name: LLM_MODEL_NAME
          value: "phi"     # Default to phi model
        - name: LLM_BASE_URL
          value: "http://localhost:11434"  # Ollama sidecar
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        resources:
          requests:
            memory: "256Mi"   # Increased for LLM processing
            cpu: "200m"       # Increased for LLM processing
          limits:
            memory: "512Mi"   # Increased for LLM processing
            cpu: "500m"       # Increased for LLM processing
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 120  # Longer for model loading
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 60   # Longer for model loading
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        startupProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 30
          periodSeconds: 15
          timeoutSeconds: 10
          failureThreshold: 20  # Much longer for model loading
        volumeMounts:
        - name: app-logs
          mountPath: /app/logs
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          capabilities:
            drop:
            - ALL
      volumes:
      - name: app-logs
        emptyDir: {}
      - name: ollama-data
        emptyDir:
          sizeLimit: 10Gi  # Space for models
      terminationGracePeriodSeconds: 60  # Longer for graceful model shutdown
      restartPolicy: Always 