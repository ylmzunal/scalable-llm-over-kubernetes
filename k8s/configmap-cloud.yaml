apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-chatbot-config
  namespace: default
  labels:
    app: llm-chatbot
    environment: cloud
data:
  # LLM Configuration - Cloud Deployment (Local Ollama Models)
  model_provider: "ollama"  # ollama for cloud deployment with local models
  model_name: "phi"         # Fast phi model for cloud deployment
  
  # Ollama Configuration (Sidecar container)
  llm_base_url: "http://localhost:11434"
  
  # Local Models Available in Cloud:
  # - phi (2.7B) - Fast and efficient
  # - llama2:7b (7B) - More capable but slower
  
  # Application Configuration
  log_level: "INFO"
  max_connections: "50"  # Reduced for cloud limits
  connection_timeout: "30"
  
  # Feature Flags
  enable_websockets: "true"
  enable_metrics: "true"
  enable_health_checks: "true"
  enable_model_switching: "false"  # Disabled in cloud
  
  # Performance Settings - Optimized for Cloud with Ollama
  worker_processes: "1"
  max_requests_per_worker: "200"  # Reduced for Ollama resource constraints
  keep_alive_timeout: "65"

---
# Secret for optional configurations
apiVersion: v1
kind: Secret
metadata:
  name: llm-chatbot-secrets
  namespace: default
  labels:
    app: llm-chatbot
    environment: cloud
type: Opaque
data:
  # Currently no secrets needed for Ollama deployment
  # Future use for API keys or certificates 